{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f001a4",
   "metadata": {},
   "source": [
    "# KNHANES 2014–2017: Data Prep Pipeline (ALL ↔ PAM)\n",
    "\n",
    "본 노트북은 분석 재현을 위해 **수집→정제→변환→병합** 4단계 파이프라인을\n",
    "Colab/로컬 공용으로 제공합니다. 각 단계는 실행 시 **shape/columns 로그**를 출력합니다.\n",
    "\n",
    "**산출물**\n",
    "- `csv/2014_all.csv.gz` … `csv/2017_all.csv.gz` (존재 시 재생성하지 않음)\n",
    "- `csv/health_2014_2017.csv.gz` (건강 표준 테이블)\n",
    "- `csv/analysis_ready_expanded.csv.gz` (최종 분석 테이블)\n",
    "\n",
    "> 참고: PAM 요약(`csv/20xx_pam.csv.gz`)이 없으면 health만 기반으로 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca5b1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadstat\n",
      "  Downloading pyreadstat-1.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting narwhals>=2.0 (from pyreadstat)\n",
      "  Downloading narwhals-2.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /Users/ohseoyoung/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pyreadstat) (2.2.6)\n",
      "Downloading pyreadstat-1.3.1-cp311-cp311-macosx_11_0_arm64.whl (587 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.2.0-py3-none-any.whl (401 kB)\n",
      "Installing collected packages: narwhals, pyreadstat\n",
      "\u001b[2K  Attempting uninstall: narwhals\n",
      "\u001b[2K    Found existing installation: narwhals 1.47.0\n",
      "\u001b[2K    Uninstalling narwhals-1.47.0:\n",
      "\u001b[2K      Successfully uninstalled narwhals-1.47.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pyreadstat]\n",
      "\u001b[1A\u001b[2KSuccessfully installed narwhals-2.2.0 pyreadstat-1.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyreadstat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad8b287",
   "metadata": {},
   "source": [
    "## 1) SAS7BDAT → CSV.GZ 변환\n",
    "\n",
    "- SAS 원본(`hn14_all.sas7bdat` 등)이 있을 때만 변환합니다.\n",
    "- 이미 `csv/*_all.csv.gz`가 있으면 스킵합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9acefe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, gc\n",
    "import pandas as pd\n",
    "\n",
    "# 반드시 설치되어 있어야 합니다 (한 번만)\n",
    "# pip install pyreadstat\n",
    "import pyreadstat\n",
    "\n",
    "ENCODING_TRY = [None, \"cp949\", \"euc-kr\", \"latin1\"]  # 한글 우선, 마지막은 넓은 범위\n",
    "\n",
    "def read_sas7bdat_robust(sas_path: str):\n",
    "    \"\"\"\n",
    "    pyreadstat로 sas7bdat을 읽되, 인코딩을 바꿔가며 재시도\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for enc in ENCODING_TRY:\n",
    "        try:\n",
    "            df, meta = pyreadstat.read_sas7bdat(sas_path, encoding=enc)\n",
    "            print(f\"  [OK] read_sas7bdat | encoding={enc}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"  [RETRY] encoding={enc} -> {type(e).__name__}: {e}\")\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "def convert_sas_to_csv(sas_path: str, out_path: str, keep_cols=None):\n",
    "    p = Path(sas_path)\n",
    "    if not p.exists():\n",
    "        print(f\"[SKIP] Not found: {sas_path}\")\n",
    "        return False\n",
    "    if p.stat().st_size == 0:\n",
    "        print(f\"[ERROR] Empty file: {sas_path}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"\\n[READ] {sas_path} (size={p.stat().st_size:,} bytes)\")\n",
    "    try:\n",
    "        df = read_sas7bdat_robust(sas_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] failed to read: {e}\")\n",
    "        return False\n",
    "\n",
    "    # 필요한 열만 저장하고 싶으면 keep_cols로 제한\n",
    "    if keep_cols:\n",
    "        found = [c for c in keep_cols if c in df.columns]\n",
    "        if found:\n",
    "            df = df[found]\n",
    "        else:\n",
    "            print(\"  [WARN] keep_cols가 데이터에 없어 전체 컬럼 저장합니다.\")\n",
    "\n",
    "    print(f\"  shape={df.shape} | cols={len(df.columns)}\")\n",
    "    print(\"  head:\\n\", df.head(3))\n",
    "\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_path, index=False, compression=\"gzip\")\n",
    "    print(f\"[SAVE] -> {out_path} | rows={len(df)}\")\n",
    "    del df\n",
    "    gc.collect()\n",
    "    return True\n",
    "\n",
    "# ----------------- 실행부 예시 -----------------\n",
    "YEARS = [2014, 2015, 2016, 2017]\n",
    "SAS_ALLS = {\n",
    "    2014: \"hn14_all.sas7bdat\",\n",
    "    2015: \"hn15_all.sas7bdat\",\n",
    "    2016: \"hn16_all.sas7bdat\",\n",
    "    2017: \"hn17_all.sas7bdat\",\n",
    "}\n",
    "SAS_PAMS = {\n",
    "    2014: \"hn14_pam.sas7bdat\",\n",
    "    2015: \"hn15_pam.sas7bdat\",\n",
    "    2016: \"hn16_pam.sas7bdat\",\n",
    "    2017: \"hn17_pam.sas7bdat\",\n",
    "}\n",
    "PATH_ALLS = {y: f\"csv/{y}_all.csv.gz\" for y in YEARS}\n",
    "PATH_PAMS = {y: f\"csv/{y}_pam.csv.gz\" for y in YEARS}\n",
    "\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "\n",
    "# ALL 변환\n",
    "for y in YEARS:\n",
    "    convert_sas_to_csv(SAS_ALLS[y], PATH_ALLS[y])\n",
    "\n",
    "# PAM 변환\n",
    "for y in YEARS:\n",
    "    convert_sas_to_csv(SAS_PAMS[y], PATH_PAMS[y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0fed44",
   "metadata": {},
   "source": [
    "## 2) ALL → 건강표준 테이블 (정제·변환)\n",
    "\n",
    "- 컬럼 표준화(MAP_ALL)\n",
    "- 파생: 비만/고혈압/당뇨, sex_female 등\n",
    "- 로그: 전체 병합 shape, 저장 경로\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fac3945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_health_table():\n",
    "    parts = []\n",
    "    for y, p in PATH_ALLS.items():\n",
    "        if os.path.exists(p):\n",
    "            d = read_all_standardized(p); parts.append(d)\n",
    "        else:\n",
    "            print(f\"[WARN] ALL not found, skip: {p}\")\n",
    "    if not parts:\n",
    "        raise RuntimeError(\"No ALL files to build health table.\")\n",
    "    all_df = pd.concat(parts, ignore_index=True)\n",
    "    print(\"ALL merged shape:\", all_df.shape)\n",
    "    print(\"ALL columns (head):\", list(all_df.columns)[:12])\n",
    "\n",
    "    dfh = all_df.copy()\n",
    "    if \"sex\" in dfh: dfh[\"sex_female\"] = (dfh[\"sex\"] == 2).astype(int)\n",
    "    if \"age\" in dfh: dfh[\"age\"] = pd.to_numeric(dfh[\"age\"], errors=\"coerce\")\n",
    "\n",
    "    # 안전하게 시리즈 생성\n",
    "    SBP = dfh[\"SBP\"] if \"SBP\" in dfh else pd.Series(np.nan, index=dfh.index)\n",
    "    DBP = dfh[\"DBP\"] if \"DBP\" in dfh else pd.Series(np.nan, index=dfh.index)\n",
    "    htn_med = dfh[\"htn_med\"] if \"htn_med\" in dfh else pd.Series(0, index=dfh.index)\n",
    "\n",
    "    HbA1c = dfh[\"HbA1c\"] if \"HbA1c\" in dfh else pd.Series(np.nan, index=dfh.index)\n",
    "    dm_med = dfh[\"dm_med\"] if \"dm_med\" in dfh else pd.Series(0, index=dfh.index)\n",
    "\n",
    "    # Outcomes\n",
    "    dfh[\"out_obesity\"] = (dfh[\"BMI\"] >= 30).astype(int) if \"BMI\" in dfh else pd.Series(np.nan, index=dfh.index)\n",
    "    dfh[\"out_hypertension\"] = ((SBP >= 140) | (DBP >= 90) | (htn_med == 1)).astype(int)\n",
    "    dfh[\"out_diabetes\"] = ((HbA1c >= 6.5) | (dm_med == 1)).astype(int)\n",
    "\n",
    "    # Behavior\n",
    "    dfh[\"smoker\"] = (dfh[\"smoking\"] > 0).astype(int) if \"smoking\" in dfh else pd.Series(np.nan, index=dfh.index)\n",
    "    dfh[\"alcohol_freq\"] = dfh[\"alcohol\"] if \"alcohol\" in dfh else pd.Series(np.nan, index=dfh.index)\n",
    "    if \"kcal\" in dfh: dfh[\"kcal\"] = pd.to_numeric(dfh[\"kcal\"], errors=\"coerce\")\n",
    "\n",
    "    keep = [\"ID\",\"year\",\"sex_female\",\"age\",\"smoker\",\"alcohol_freq\",\"kcal\",\n",
    "            \"BMI\",\"SBP\",\"DBP\",\"HbA1c\",\n",
    "            \"out_obesity\",\"out_hypertension\",\"out_diabetes\",\n",
    "            \"incm\",\"edu\",\"wt_tot\",\"psu\",\"kstrata\"]\n",
    "    dfh = dfh[[c for c in keep if c in dfh.columns]].dropna(subset=[\"age\"])\n",
    "\n",
    "    outp = \"csv/health_2014_2017.csv.gz\"\n",
    "    dfh.to_csv(outp, index=False, compression=\"gzip\")\n",
    "    print(f\"[Saved] {outp} | shape={dfh.shape}\")\n",
    "    print(\"Health columns:\", list(dfh.columns))\n",
    "    return dfh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dee80d",
   "metadata": {},
   "source": [
    "## 3) PAM 요약 불러오기 & 4) 병합\n",
    "\n",
    "- 개인 단위(PID, year) 요약이 존재하면 병합, 없으면 health만 저장\n",
    "- 해석 편의를 위한 스케일링: `mvpa10`(10분 단위), `sed10`(10%p 단위)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "532fc426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAM person shape: (28392322, 2)\n",
      "PAM columns: ['ID', 'year']\n",
      "[Saved] csv/analysis_ready_expanded.csv.gz | shape=(24259522, 14)\n",
      "Final columns (head): ['ID', 'year', 'sex_female', 'age', 'smoker', 'alcohol_freq', 'out_obesity', 'out_hypertension', 'out_diabetes', 'incm', 'edu', 'wt_tot', 'psu', 'kstrata']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>year</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>age</th>\n",
       "      <th>smoker</th>\n",
       "      <th>alcohol_freq</th>\n",
       "      <th>out_obesity</th>\n",
       "      <th>out_hypertension</th>\n",
       "      <th>out_diabetes</th>\n",
       "      <th>incm</th>\n",
       "      <th>edu</th>\n",
       "      <th>wt_tot</th>\n",
       "      <th>psu</th>\n",
       "      <th>kstrata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A209799515</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5573.985554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>612.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A209799515</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5573.985554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>612.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A209799515</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5573.985554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>612.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A209799515</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5573.985554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>612.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A209799515</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5573.985554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>612.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  year  sex_female   age  smoker  alcohol_freq  out_obesity  out_hypertension  out_diabetes  incm  edu       wt_tot  psu  kstrata\n",
       "0  A209799515  2014           1  64.0     NaN           NaN          NaN                 0             0   3.0  3.0  5573.985554  NaN    612.0\n",
       "1  A209799515  2014           1  64.0     NaN           NaN          NaN                 0             0   3.0  3.0  5573.985554  NaN    612.0\n",
       "2  A209799515  2014           1  64.0     NaN           NaN          NaN                 0             0   3.0  3.0  5573.985554  NaN    612.0\n",
       "3  A209799515  2014           1  64.0     NaN           NaN          NaN                 0             0   3.0  3.0  5573.985554  NaN    612.0\n",
       "4  A209799515  2014           1  64.0     NaN           NaN          NaN                 0             0   3.0  3.0  5573.985554  NaN    612.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_pam_person():\n",
    "    items = []\n",
    "    for y, p in PATH_PAM.items():\n",
    "        if os.path.exists(p):\n",
    "            d = pd.read_csv(p)\n",
    "            ren = {}\n",
    "            for c in d.columns:\n",
    "                cl = str(c).lower()\n",
    "                if cl == \"id\": ren[c] = \"ID\"\n",
    "                if cl == \"year\": ren[c] = \"year\"\n",
    "            if ren: d = d.rename(columns=ren)\n",
    "            need = [\"ID\",\"year\",\"n_days\",\"worn_min_day\",\"mvpa_min_day\",\"sed_ratio\"]\n",
    "            d = d[[c for c in need if c in d.columns]].copy()\n",
    "            d[\"ID\"] = d[\"ID\"].astype(str)\n",
    "            items.append(d)\n",
    "        else:\n",
    "            print(f\"[PAM] 없음: {p}\")\n",
    "    if not items:\n",
    "        return None\n",
    "    act = pd.concat(items, ignore_index=True)\n",
    "    print(\"PAM person shape:\", act.shape)\n",
    "    print(\"PAM columns:\", list(act.columns))\n",
    "    uniq = act[\"worn_min_day\"].round(1).nunique() if \"worn_min_day\" in act else None\n",
    "    if uniq is not None and uniq <= 2:\n",
    "        print(f\"[Note] worn_min_day 분산 낮음 (unique={uniq})\")\n",
    "    return act\n",
    "\n",
    "def build_analysis_ready():\n",
    "    act = read_pam_person()\n",
    "    hlth = pd.read_csv(\"csv/health_2014_2017.csv.gz\")\n",
    "    for d in (act, hlth):\n",
    "        if d is None: continue\n",
    "        if \"id\" in d.columns and \"ID\" not in d.columns:\n",
    "            d.rename(columns={\"id\":\"ID\"}, inplace=True)\n",
    "        if \"year\" in d.columns:\n",
    "            d[\"year\"] = pd.to_numeric(d[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        d[\"ID\"] = d[\"ID\"].astype(str)\n",
    "\n",
    "    if act is not None:\n",
    "        df = act.merge(hlth, on=[\"ID\",\"year\"], how=\"inner\")\n",
    "    else:\n",
    "        print(\"[Note] PAM 요약 없음 → health만 사용\")\n",
    "        df = hlth.copy()\n",
    "\n",
    "    if \"mvpa_min_day\" in df: df[\"mvpa10\"] = df[\"mvpa_min_day\"] / 10.0\n",
    "    if \"sed_ratio\" in df:    df[\"sed10\"]  = df[\"sed_ratio\"] * 10.0\n",
    "\n",
    "    # 가중치/설계 존재 확인\n",
    "    if \"wt_tot\" not in df.columns:\n",
    "        print(\"[HOTFIX] 가중치 미존재 → wt_tot=1.0 설정\")\n",
    "        df[\"wt_tot\"] = 1.0\n",
    "    if not {\"psu\",\"kstrata\"}.issubset(df.columns):\n",
    "        print(\"[HOTFIX] psu/kstrata 없음 → 설계부트 불가(후속분석 폴백)\")\n",
    "\n",
    "    outp = \"csv/analysis_ready_expanded.csv.gz\"\n",
    "    df.to_csv(outp, index=False, compression=\"gzip\")\n",
    "    print(f\"[Saved] {outp} | shape={df.shape}\")\n",
    "    print(\"Final columns (head):\", list(df.columns)[:20])\n",
    "    return df\n",
    "\n",
    "df_final = build_analysis_ready()\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a55205d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL merged shape: (25131, 9)\n",
      "ALL columns (head): ['ID', 'year', 'sex', 'age', 'incm', 'edu', 'wt_tot', 'psu', 'kstrata']\n",
      "[Saved] csv/health_2014_2017.csv.gz | shape=(25131, 14)\n",
      "Health columns: ['ID', 'year', 'sex_female', 'age', 'smoker', 'alcohol_freq', 'out_obesity', 'out_hypertension', 'out_diabetes', 'incm', 'edu', 'wt_tot', 'psu', 'kstrata']\n"
     ]
    }
   ],
   "source": [
    "dfh = build_health_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78cc949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ========== 유틸 ==========\n",
    "def _find_col(cols, candidates):\n",
    "    \"\"\"후보 목록 중 데이터프레임에 존재하는 첫 컬럼명을 반환(대소문자 무시)\"\"\"\n",
    "    low = {c.lower(): c for c in cols}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in low:\n",
    "            return low[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def _to_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "# ========== 핵심: minute → person 요약 ==========\n",
    "def build_activity_person(path_pam_map,\n",
    "                          out_person_path=\"csv/activity_person_2014_2017.csv.gz\",\n",
    "                          chunk_size=2_000_000,\n",
    "                          min_wear_per_day=600,   # ≥10h/day\n",
    "                          min_valid_days=4):      # ≥4일\n",
    "    \"\"\"\n",
    "    Minute-level PAM CSV(.gz)들을 읽어 개인 요약치(n_days, worn_min_day, mvpa_min_day, sed_ratio)로 변환.\n",
    "    - 입력: path_pam_map = {2014: 'csv/2014_pam.csv.gz', ...}\n",
    "    - 출력: out_person_path 저장 및 DataFrame 리턴\n",
    "    \"\"\"\n",
    "    Path(\"csv\").mkdir(exist_ok=True)\n",
    "    per_year_person = []\n",
    "\n",
    "    for year, path in path_pam_map.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"[PAM] 파일 없음: {path} (연도 {year}) → 스킵\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[PAM] {year} 처리 시작 → {path}\")\n",
    "        # 1) 컬럼 샘플링해서 스키마 파악\n",
    "        sample = pd.read_csv(path, nrows=50)\n",
    "        # 기본키\n",
    "        id_col   = _find_col(sample.columns, [\"ID\", \"id\", \"Id\"])\n",
    "        year_col = _find_col(sample.columns, [\"year\", \"Year\"])\n",
    "        # 날짜/일자\n",
    "        date_col = _find_col(sample.columns, [\"date\", \"day\", \"mod_d\", \"DATE\", \"Day\"])\n",
    "        # 분 단위 지표(원-핫 또는 인디케이터)\n",
    "        sed_col  = _find_col(sample.columns, [\"sed\", \"sedentary\", \"SED\"])\n",
    "        lgt_col  = _find_col(sample.columns, [\"light\", \"lgt\", \"LIGHT\"])\n",
    "        mod_col  = _find_col(sample.columns, [\"mod\", \"moderate\", \"MOD\"])\n",
    "        vig_col  = _find_col(sample.columns, [\"vig\", \"vigorous\", \"VIG\"])\n",
    "        # 혹시 per-minute 카운트가 있다면(없어도 됨)\n",
    "        cnt_col  = _find_col(sample.columns, [\"counts\", \"cpm\", \"cnt\"])\n",
    "\n",
    "        # 필수키 확인\n",
    "        assert id_col and year_col, f\"[{path}] ID/year 컬럼을 찾지 못했습니다.\"\n",
    "        assert date_col, f\"[{path}] 날짜(일자) 컬럼(date/day/mod_d 등)을 찾지 못했습니다.\"\n",
    "        assert (sed_col or lgt_col or mod_col or vig_col), f\"[{path}] sed/light/mod/vig 중 최소 1개가 필요합니다.\"\n",
    "\n",
    "        usecols = [id_col, year_col, date_col]\n",
    "        for c in [sed_col, lgt_col, mod_col, vig_col, cnt_col]:\n",
    "            if c and c not in usecols:\n",
    "                usecols.append(c)\n",
    "\n",
    "        # 2) chunk 단위로 읽어서 '개인-일자' 집계\n",
    "        daily_accum = []  # (ID, year, date, wear_min, sed_min, mvpa_min)\n",
    "\n",
    "        total_rows = 0\n",
    "        for chunk in pd.read_csv(path, usecols=usecols, chunksize=chunk_size):\n",
    "            total_rows += len(chunk)\n",
    "            # 타입/정리\n",
    "            chunk[id_col]   = chunk[id_col].astype(str)\n",
    "            chunk[year_col] = _to_num(chunk[year_col]).astype(\"Int64\")\n",
    "\n",
    "            # 날짜 → 날짜형(일 단위만 유지)\n",
    "            # mod_d 처럼 문자열(YYYY.MM.DD. or YYYY-MM-DD 등)도 처리가능\n",
    "            chunk[date_col] = pd.to_datetime(chunk[date_col], errors=\"coerce\").dt.date\n",
    "\n",
    "            # 분단위 지표 0/1 보정\n",
    "            for c in [sed_col, lgt_col, mod_col, vig_col]:\n",
    "                if c:\n",
    "                    chunk[c] = _to_num(chunk[c]).fillna(0).clip(lower=0)\n",
    "                    # 값이 0/1이 아니어도 0/1로 수렴(>0 → 1)\n",
    "                    chunk[c] = (chunk[c] > 0).astype(int)\n",
    "\n",
    "            # 착용 분(wear): sed|light|mod|vig 중 1이라도 1이면 착용으로 간주\n",
    "            wear_arr = None\n",
    "            for c in [sed_col, lgt_col, mod_col, vig_col]:\n",
    "                if c:\n",
    "                    wear_arr = chunk[c] if wear_arr is None else (wear_arr | (chunk[c] == 1))\n",
    "            chunk[\"_wear\"] = wear_arr.astype(int)\n",
    "\n",
    "            # MVPA = mod + vig (분)\n",
    "            mvpa_arr = None\n",
    "            for c in [mod_col, vig_col]:\n",
    "                if c:\n",
    "                    mvpa_arr = chunk[c] if mvpa_arr is None else (mvpa_arr + chunk[c])\n",
    "            if mvpa_arr is None:\n",
    "                mvpa_arr = 0\n",
    "            chunk[\"_mvpa\"] = mvpa_arr\n",
    "\n",
    "            # sed 분\n",
    "            chunk[\"_sed\"] = chunk[sed_col] if sed_col else 0\n",
    "\n",
    "            # 일자 집계\n",
    "            g = chunk.groupby([id_col, year_col, date_col], as_index=False).agg(\n",
    "                wear_min=(\"_wear\", \"sum\"),\n",
    "                sed_min =(\"_sed\",  \"sum\"),\n",
    "                mvpa_min=(\"_mvpa\", \"sum\"),\n",
    "            )\n",
    "            daily_accum.append(g)\n",
    "\n",
    "            # 메모리\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "\n",
    "            print(f\"  - processed rows: {total_rows:,}\", end=\"\\r\")\n",
    "\n",
    "        if not daily_accum:\n",
    "            print(f\"[WARN] {year}: daily_accum 비어있음 → 스킵\")\n",
    "            continue\n",
    "\n",
    "        daily = pd.concat(daily_accum, ignore_index=True)\n",
    "        # 3) 유효일 기준 필터(≥10h/day)\n",
    "        daily = daily[daily[\"wear_min\"] >= min_wear_per_day].copy()\n",
    "\n",
    "        # 4) 개인 요약: 일수, 평균 착용분, 평균 MVPA, 좌식비율(총 sed / 총 wear)\n",
    "        person = (\n",
    "            daily\n",
    "            .groupby([id_col, year_col], as_index=False)\n",
    "            .agg(\n",
    "                n_days     = (\"wear_min\", \"size\"),\n",
    "                worn_total = (\"wear_min\", \"sum\"),\n",
    "                sed_total  = (\"sed_min\",  \"sum\"),\n",
    "                mvpa_total = (\"mvpa_min\", \"sum\"),\n",
    "            )\n",
    "        )\n",
    "        person[\"worn_min_day\"] = person[\"worn_total\"] / person[\"n_days\"]\n",
    "        person[\"mvpa_min_day\"] = person[\"mvpa_total\"] / person[\"n_days\"]\n",
    "        person[\"sed_ratio\"]    = np.where(person[\"worn_total\"] > 0,\n",
    "                                          person[\"sed_total\"] / person[\"worn_total\"],\n",
    "                                          np.nan)\n",
    "        # 유효일수 기준(≥4일) 필터\n",
    "        person = person[person[\"n_days\"] >= min_valid_days].copy()\n",
    "\n",
    "        # 표준 컬럼명 정리\n",
    "        person = person.rename(columns={id_col: \"ID\", year_col: \"year\"})\n",
    "        person = person[[\"ID\",\"year\",\"n_days\",\"worn_min_day\",\"mvpa_min_day\",\"sed_ratio\"]].copy()\n",
    "        person[\"ID\"] = person[\"ID\"].astype(str)\n",
    "        person[\"year\"] = person[\"year\"].astype(\"Int64\")\n",
    "\n",
    "        # 연도별 저장(선택)\n",
    "        out_y = f\"csv/{year}_pam_person.csv.gz\"\n",
    "        person.to_csv(out_y, index=False, compression=\"gzip\")\n",
    "        print(f\"[SAVE] {year} → {out_y} | rows={len(person):,}\")\n",
    "\n",
    "        per_year_person.append(person)\n",
    "\n",
    "        # 큰 객체 해제\n",
    "        del daily_accum, daily, person\n",
    "        gc.collect()\n",
    "\n",
    "    if not per_year_person:\n",
    "        raise RuntimeError(\"모든 연도에서 person-level 요약이 생성되지 않았습니다.\")\n",
    "\n",
    "    act = pd.concat(per_year_person, ignore_index=True)\n",
    "    act.to_csv(out_person_path, index=False, compression=\"gzip\")\n",
    "    print(f\"[DONE] person-level 통합 저장 → {out_person_path} | rows={len(act):,}\")\n",
    "    print(\"Columns:\", list(act.columns))\n",
    "    return act\n",
    "\n",
    "# ========== 기존 통합 단계와 연결(업데이트) ==========\n",
    "def read_pam_person_safely():\n",
    "    merged_path = \"csv/activity_person_2014_2017.csv.gz\"\n",
    "    if os.path.exists(merged_path):\n",
    "        d = pd.read_csv(merged_path, dtype={\"ID\":\"string\"})\n",
    "        print(f\"[LOAD] {merged_path} | shape={d.shape}\")\n",
    "        return d\n",
    "    else:\n",
    "        print(\"[MISS] person-level PAM 통합 파일 없음 → 즉시 생성\")\n",
    "        return build_activity_person(PATH_PAM)\n",
    "\n",
    "def build_analysis_ready_updated():\n",
    "    # 1) person-level PAM\n",
    "    act = read_pam_person_safely()\n",
    "\n",
    "    # 2) health 테이블\n",
    "    hlth_path = \"csv/health_2014_2017.csv.gz\"\n",
    "    assert os.path.exists(hlth_path), \"health_2014_2017.csv.gz가 필요합니다. 먼저 build_health_table()을 실행하세요.\"\n",
    "    hlth = pd.read_csv(hlth_path, dtype={\"ID\":\"string\"})\n",
    "    # 타입 정리\n",
    "    for d in (act, hlth):\n",
    "        if \"year\" in d.columns:\n",
    "            d[\"year\"] = pd.to_numeric(d[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        d[\"ID\"] = d[\"ID\"].astype(str)\n",
    "\n",
    "    # 3) 병합\n",
    "    df = act.merge(hlth, on=[\"ID\",\"year\"], how=\"inner\")\n",
    "    print(\"Merged shape:\", df.shape)\n",
    "\n",
    "    # 4) 노출 스케일\n",
    "    if \"mvpa_min_day\" in df: df[\"mvpa10\"] = df[\"mvpa_min_day\"] / 10.0\n",
    "    if \"sed_ratio\" in df:    df[\"sed10\"]  = df[\"sed_ratio\"] * 10.0\n",
    "\n",
    "    # 5) 가중치/설계 폴백\n",
    "    if \"wt_tot\" not in df.columns:\n",
    "        print(\"[HOTFIX] 가중치 미존재 → wt_tot=1.0\")\n",
    "        df[\"wt_tot\"] = 1.0\n",
    "    if not {\"psu\",\"kstrata\"}.issubset(df.columns):\n",
    "        print(\"[HOTFIX] psu/kstrata 없음 → 설계부트 불가(후속분석 폴백)\")\n",
    "\n",
    "    outp = \"csv/analysis_ready_expanded.csv.gz\"\n",
    "    df.to_csv(outp, index=False, compression=\"gzip\")\n",
    "    print(f\"[SAVE] {outp} | shape={df.shape}\")\n",
    "    print(\"Cols(head):\", list(df.columns)[:20])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14274a7",
   "metadata": {},
   "source": [
    "## (참고) 우리가 겪었던 대표 이슈 & 가드\n",
    "\n",
    "- **Series→bool 축소**로 `.astype(int)` 에러 → 연산 전 컬럼 존재 여부 확인\n",
    "- **사분위수 분할 실패**(`Bin edges must be unique`) → 착용시간 분산 낮을 때 `duplicates=\"drop\"` 또는 분할 스킵\n",
    "- **열명 대소문자/이형** → lower-case 매칭으로 표준화\n",
    "- **설계정보 부재** → survey 설계기반 SE 불가 → 폴백으로 단순가중/부트스트랩\n",
    "- **디렉터리 누락** → `csv/`, `out/` 사전 생성\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
